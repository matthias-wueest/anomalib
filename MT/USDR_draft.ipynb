{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/wueesmat/MT/anomalib')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set path\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_directory = Path.cwd()\n",
    "if current_directory.name == \"MT\":\n",
    "    # This means that the notebook is run from the main anomalib directory.\n",
    "    root_directory = current_directory.parent\n",
    "elif current_directory.name == \"anomalib\":\n",
    "    # This means that the notebook is run from the main anomalib directory.\n",
    "    root_directory = current_directory\n",
    "    print(\"here\")\n",
    "\n",
    "os.chdir(root_directory)\n",
    "root_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find wandb. To use this feature, ensure that you have wandb installed.\n",
      "Could not find openvino. To use this feature, ensure that you have openvino installed.\n",
      "OpenVINO is not installed. Please install OpenVINO to use OpenVINOInferencer.\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from lightning.pytorch import seed_everything\n",
    "from anomalib.data.image.mvtec import MVTec_contaminated, MVTecDataset_contaminated\n",
    "from anomalib.models import Patchcore\n",
    "from anomalib.engine import Engine\n",
    "from anomalib import TaskType\n",
    "from anomalib.data.utils import Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_and_predict_on_training_set_blind(coreset_sampling_ratio, run, category, cont_ratio):\n",
    "#    seed_everything(run, workers=True)\n",
    "#    datamodule = MVTec_contaminated(category=category, cont_ratio=cont_ratio, run=run, idx=[])\n",
    "#    model = Patchcore(coreset_sampling_ratio=coreset_sampling_ratio)\n",
    "#    engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "#    engine.fit(datamodule=datamodule, model=model)\n",
    "#\n",
    "#    # Predict on complete training set\n",
    "#    seed_everything(run, workers=True)\n",
    "#    train_dataset = MVTecDataset_contaminated(\n",
    "#                    task=TaskType.CLASSIFICATION,\n",
    "#                    split=Split.TRAIN,\n",
    "#                    category=category,\n",
    "#                    cont_ratio=cont_ratio,\n",
    "#                    run=run,\n",
    "#                    idx = []           \n",
    "#                )\n",
    "#    predictions_subset = engine.predict(model=model, dataset=train_dataset)\n",
    "#    \n",
    "#    return predictions_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_on_training_set_blind(coreset_sampling_ratio, run, category, cont_ratio):\n",
    "    seed_everything(run, workers=True)\n",
    "    datamodule = MVTec_contaminated(category=category, cont_ratio=cont_ratio, run=run, idx=[])\n",
    "    model = Patchcore(coreset_sampling_ratio=coreset_sampling_ratio)\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "    engine.fit(datamodule=datamodule, model=model)\n",
    "\n",
    "    # Predict on complete training set\n",
    "    seed_everything(run, workers=True)\n",
    "    train_dataset = MVTecDataset_contaminated(\n",
    "                    task=TaskType.CLASSIFICATION,\n",
    "                    split=Split.TRAIN,\n",
    "                    category=category,\n",
    "                    cont_ratio=cont_ratio,\n",
    "                    run=run,\n",
    "                    idx = []           \n",
    "                )\n",
    "    predictions = engine.predict(model=model, dataset=train_dataset)\n",
    "    S_blind = np.array([d[\"pred_scores\"][0] for d in predictions])\n",
    "    S_blind = minmax_scale(S_blind, feature_range=(0,1))\n",
    "    Y_blind = np.array([item['label'].item() for item in predictions])\n",
    "    \n",
    "    return S_blind, Y_blind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_on_training_set_clean(coreset_sampling_ratio, run, category, cont_ratio):\n",
    "    seed_everything(run, workers=True)\n",
    "    datamodule = MVTec_contaminated(category=category, cont_ratio=0, run=run, idx=[])\n",
    "    model = Patchcore(coreset_sampling_ratio=coreset_sampling_ratio)\n",
    "    engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "    engine.fit(datamodule=datamodule, model=model)\n",
    "\n",
    "    # Predict on complete training set\n",
    "    seed_everything(run, workers=True)\n",
    "    train_dataset = MVTecDataset_contaminated(\n",
    "                    task=TaskType.CLASSIFICATION,\n",
    "                    split=Split.TRAIN,\n",
    "                    category=category,\n",
    "                    cont_ratio=cont_ratio,\n",
    "                    run=run,\n",
    "                    idx = []           \n",
    "                )\n",
    "    predictions = engine.predict(model=model, dataset=train_dataset)\n",
    "    S_clean = np.array([d[\"pred_scores\"][0] for d in predictions])\n",
    "    S_clean = minmax_scale(S_clean, feature_range=(0,1))\n",
    "    Y_clean = np.array([item['label'].item() for item in predictions])\n",
    "\n",
    "    return S_clean, Y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_on_training_set_SRR_light(coreset_sampling_ratio, run, category, cont_ratio):\n",
    "\n",
    "    k = 5\n",
    "    gamma = 1 - cont_ratio\n",
    "\n",
    "    train_dataset = MVTecDataset_contaminated(\n",
    "                    task=TaskType.CLASSIFICATION,\n",
    "                    split=Split.TRAIN,\n",
    "                    category=category,\n",
    "                    cont_ratio=cont_ratio,\n",
    "                    run=run,\n",
    "                    idx = []           \n",
    "                )\n",
    "\n",
    "    \n",
    "    # Create indices for k disjoint datasets\n",
    "    train_dataset_length = train_dataset.__len__()\n",
    "    print(\"train_dataset_length:\", train_dataset_length)\n",
    "    indices = np.arange(0, train_dataset_length)\n",
    "    np.random.seed(run)\n",
    "    np.random.shuffle(indices)\n",
    "    indices_disjoint_datasets = np.array_split(indices, k)\n",
    "    print(\"indices_disjoint_datasets: \", indices_disjoint_datasets)\n",
    "\n",
    "    # Train k models on k disjoint datasets\n",
    "    classifications_subset_arr = np.empty([train_dataset_length,k], dtype=bool)\n",
    "    for k_iter in range(k):\n",
    "        print(\"k_iter: \", k_iter)\n",
    "        # Train model on disjoint dataset\n",
    "        seed_everything(run, workers=True)\n",
    "        datamodule = MVTec_contaminated(category=category, cont_ratio=cont_ratio, run=run, idx=indices_disjoint_datasets[k_iter])\n",
    "        model = Patchcore(coreset_sampling_ratio=coreset_sampling_ratio) \n",
    "        engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "        engine.fit(datamodule=datamodule, model=model)\n",
    "\n",
    "        # Predict binary labels for each sample\n",
    "        predictions_subset = engine.predict(model=model, dataset=train_dataset)\n",
    "        prediction_scores_subset = np.array([d[\"pred_scores\"][0] for d in predictions_subset])\n",
    "        print(\"prediction_scores_subset: \", prediction_scores_subset)\n",
    "        print(\"gamma:\", gamma)\n",
    "        threshold = np.percentile(prediction_scores_subset, q=gamma*100)\n",
    "        print(\"threshold: \", threshold)\n",
    "        classifications_subset = prediction_scores_subset>threshold # True: abnormal; False: normal\n",
    "        print(\"classifications_subset: \", classifications_subset)\n",
    "        # Save binary classifications\n",
    "        classifications_subset_arr[:,k_iter] = classifications_subset\n",
    "\n",
    "    # Return indices of refined dataset\n",
    "    keep_bool_arr = np.all(~classifications_subset_arr, axis=1)\n",
    "    keep_indices = np.where(keep_bool_arr)[0]\n",
    "    print(\"keep_indices: \", keep_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Extract labels\n",
    "    Y_SRR = np.array([item['label'].item() for item in predictions_subset[0]])\n",
    "    Y_hat = np.where(~keep_bool_arr)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return S_SRR, Y_SRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_on_training_set_USDR(coreset_sampling_ratio, run, category, cont_ratio):\n",
    "\n",
    "    seed_everything(run, workers=True)\n",
    "    train_dataset = MVTecDataset_contaminated(\n",
    "                    task=TaskType.CLASSIFICATION,\n",
    "                    split=Split.TRAIN,\n",
    "                    category=category,\n",
    "                    cont_ratio=cont_ratio,\n",
    "                    run=run,\n",
    "                    idx = []           \n",
    "                )\n",
    "\n",
    "    # Create indices for k disjoint datasets\n",
    "    train_dataset_length = train_dataset.__len__()\n",
    "\n",
    "    # Define hyperparameters\n",
    "    N = train_dataset_length#23\n",
    "    M = 8#16\n",
    "    M_train = int(M/2)\n",
    "\n",
    "    # Create subset boolean matrix\n",
    "    ones = np.ones(M_train, dtype=bool)\n",
    "    zeros = np.zeros(M-M_train, dtype=bool)\n",
    "    period_bool = np.hstack((ones, zeros))\n",
    "    subset1_bool = np.array([], dtype=bool)\n",
    "    for i in range(int(np.ceil(N/M))):\n",
    "        subset1_bool = np.hstack((subset1_bool, period_bool))\n",
    "    subsets_bool = subset1_bool\n",
    "    for m in range(1,M):\n",
    "        subset_new_bool = np.roll(subset1_bool, shift=m)\n",
    "        subsets_bool = np.vstack((subsets_bool, subset_new_bool))\n",
    "    subsets_bool = subsets_bool[:,:int(np.floor(N/M)*M + np.mod(N, M))]\n",
    "    print(subsets_bool.astype(int))\n",
    "\n",
    "    # Initialize arrays for predictions and subsets\n",
    "    predictions_subsets_arr = np.empty(M, dtype=object)\n",
    "    subsets_bool_sorted_arr = np.empty((M,N))\n",
    "\n",
    "    # Create indices for M partially overlapping subsets\n",
    "    indices = np.arange(0, train_dataset_length)\n",
    "    np.random.seed(run)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Train M models on M partially overlapping subsets\n",
    "    for m_iter in range(M):\n",
    "\n",
    "        # Define indices for M partially overlapping subsets\n",
    "        indices_subset = np.sort(indices[np.where(subsets_bool.astype(int)[m_iter])])\n",
    "        \n",
    "        # Train model on subset\n",
    "        seed_everything(run, workers=True)\n",
    "        datamodule = MVTec_contaminated(category=category, cont_ratio=cont_ratio, run=run, idx=indices_subset)\n",
    "        model = Patchcore(coreset_sampling_ratio=coreset_sampling_ratio) \n",
    "        engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "        engine.fit(datamodule=datamodule, model=model)\n",
    "\n",
    "        # Predict complete training set\n",
    "        predictions_subset = engine.predict(model=model, dataset=train_dataset)\n",
    "\n",
    "        # Save the predictions\n",
    "        predictions_subsets_arr[m_iter] = predictions_subset\n",
    "\n",
    "        # Save the subset\n",
    "        subsets_bool_sorted = np.zeros(N)\n",
    "        subsets_bool_sorted[indices_subset] = 1\n",
    "        subsets_bool_sorted_arr[m_iter,:] = subsets_bool_sorted\n",
    "\n",
    "    # Extract 'pred_scores' from each list and stack them\n",
    "    pred_scores_list = []\n",
    "    for sublist in predictions_subsets_arr:\n",
    "        stacked_sublist_scores = np.array([d['pred_scores'].item() for d in sublist])\n",
    "        pred_scores_list.append(stacked_sublist_scores)\n",
    "    pred_scores_arr = np.vstack(pred_scores_list)\n",
    "\n",
    "    # Extract labels\n",
    "    label_arr = np.array([item['label'].item() for item in predictions_subsets_arr[0]])\n",
    "\n",
    "    # Compute USDR scores\n",
    "    Y = label_arr \n",
    "    Y_hat = pred_scores_arr\n",
    "    Residuals = abs(Y_hat) # abs(Y - Y_hat)\n",
    "    Included = subsets_bool_sorted_arr\n",
    "    NotIncluded = 1-subsets_bool_sorted_arr\n",
    "    N_j = np.sum(Included, axis=1)\n",
    "    mu_j = np.empty([M])\n",
    "    std_j = np.empty([M])\n",
    "    for j in range(M):\n",
    "        mu_j[j] = np.mean(Residuals[j,:][Included[j,:]==1])\n",
    "        std_j[j] = np.std(Residuals[j,:][Included[j,:]==1])\n",
    "    Rescaled_Residuals = (Residuals - mu_j[:, np.newaxis])/std_j[:, np.newaxis]\n",
    "    Z_train = 1/M_train*np.sum(Rescaled_Residuals*Included, axis=0)\n",
    "    Z_infer = 1/(M-M_train)*np.sum(Rescaled_Residuals*NotIncluded, axis=0)\n",
    "    S_USDR = Z_infer - Z_train\n",
    "    S_USDR = minmax_scale(S_USDR, feature_range=(0,1))\n",
    "\n",
    "    # Compute Alternative USDR score\n",
    "    Z_train_min = np.min(Rescaled_Residuals*Included, axis=0)\n",
    "    Z_infer_max = np.max(Rescaled_Residuals*NotIncluded, axis=0)\n",
    "    S_USDR_minmax = Z_infer_max - Z_train_min\n",
    "    S_USDR_minmax = minmax_scale(S_USDR_minmax, feature_range=(0,1))\n",
    "\n",
    "    # Compute Blind Ensemble Mean score\n",
    "    S_ensemble = np.mean(Rescaled_Residuals, axis=0)\n",
    "    S_ensemble = minmax_scale(S_ensemble, feature_range=(0,1))\n",
    "\n",
    "    # Compute Blind Ensemble Max score \n",
    "    S_ensemble_max = np.max(Rescaled_Residuals, axis=0)\n",
    "    S_ensemble_max = minmax_scale(S_ensemble_max, feature_range=(0,1))\n",
    "\n",
    "    return S_USDR, S_USDR_minmax, Z_infer, Z_train, Rescaled_Residuals, std_j, mu_j, Included, NotIncluded, Y_hat, Y, S_ensemble, S_ensemble_max\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BACKUP\n",
    "#def train_and_predict_on_training_set_USDR(coreset_sampling_ratio, run, category, cont_ratio):\n",
    "#\n",
    "#    seed_everything(run, workers=True)\n",
    "#    train_dataset = MVTecDataset_contaminated(\n",
    "#                    task=TaskType.CLASSIFICATION,\n",
    "#                    split=Split.TRAIN,\n",
    "#                    category=category,\n",
    "#                    cont_ratio=cont_ratio,\n",
    "#                    run=run,\n",
    "#                    idx = []           \n",
    "#                )\n",
    "#\n",
    "#    # Create indices for k disjoint datasets\n",
    "#    train_dataset_length = train_dataset.__len__()\n",
    "#\n",
    "#    # Define hyperparameters\n",
    "#    N = train_dataset_length#23\n",
    "#    M = 8#16\n",
    "#    M_train = int(M/2)\n",
    "#\n",
    "#    # Create subset boolean matrix\n",
    "#    ones = np.ones(M_train, dtype=bool)\n",
    "#    zeros = np.zeros(M-M_train, dtype=bool)\n",
    "#    period_bool = np.hstack((ones, zeros))\n",
    "#    subset1_bool = np.array([], dtype=bool)\n",
    "#    for i in range(int(np.ceil(N/M))):\n",
    "#        subset1_bool = np.hstack((subset1_bool, period_bool))\n",
    "#    subsets_bool = subset1_bool\n",
    "#    for m in range(1,M):\n",
    "#        subset_new_bool = np.roll(subset1_bool, shift=m)\n",
    "#        subsets_bool = np.vstack((subsets_bool, subset_new_bool))\n",
    "#    subsets_bool = subsets_bool[:,:int(np.floor(N/M)*M + np.mod(N, M))]\n",
    "#    print(subsets_bool.astype(int))\n",
    "#\n",
    "#    # Initialize arrays for predictions and subsets\n",
    "#    predictions_subsets_arr = np.empty(M, dtype=object)\n",
    "#    subsets_bool_sorted_arr = np.empty((M,N))\n",
    "#\n",
    "#    # Create indices for M partially overlapping subsets\n",
    "#    indices = np.arange(0, train_dataset_length)\n",
    "#    np.random.seed(run)\n",
    "#    np.random.shuffle(indices)\n",
    "#\n",
    "#    # Train M models on M partially overlapping subsets\n",
    "#    for m_iter in range(M):\n",
    "#\n",
    "#        # Define indices for M partially overlapping subsets\n",
    "#        indices_subset = np.sort(indices[np.where(subsets_bool.astype(int)[m_iter])])\n",
    "#        \n",
    "#        # Train model on subset\n",
    "#        seed_everything(run, workers=True)\n",
    "#        datamodule = MVTec_contaminated(category=category, cont_ratio=cont_ratio, run=run, idx=indices_subset)\n",
    "#        model = Patchcore(coreset_sampling_ratio=coreset_sampling_ratio) \n",
    "#        engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "#        engine.fit(datamodule=datamodule, model=model)\n",
    "#\n",
    "#        # Predict complete training set\n",
    "#        predictions_subset = engine.predict(model=model, dataset=train_dataset)\n",
    "#\n",
    "#        # Save the predictions\n",
    "#        predictions_subsets_arr[m_iter] = predictions_subset\n",
    "#\n",
    "#        # Save the subset\n",
    "#        subsets_bool_sorted = np.zeros(N)\n",
    "#        subsets_bool_sorted[indices_subset] = 1\n",
    "#        subsets_bool_sorted_arr[m_iter,:] = subsets_bool_sorted\n",
    "#\n",
    "#    # Extract 'pred_scores' from each list and stack them\n",
    "#    pred_scores_list = []\n",
    "#    for sublist in predictions_subsets_arr:\n",
    "#        stacked_sublist_scores = np.array([d['pred_scores'].item() for d in sublist])\n",
    "#        pred_scores_list.append(stacked_sublist_scores)\n",
    "#    pred_scores_arr = np.vstack(pred_scores_list)\n",
    "#\n",
    "#    # Extract labels\n",
    "#    label_arr = np.array([item['label'].item() for item in predictions_subsets_arr[0]])\n",
    "#\n",
    "#    # Compute USDR scores\n",
    "#    Y = label_arr \n",
    "#    Y_hat = pred_scores_arr\n",
    "#    Residuals = abs(Y_hat) # abs(Y - Y_hat)\n",
    "#    Included = subsets_bool_sorted_arr\n",
    "#    NotIncluded = 1-subsets_bool_sorted_arr\n",
    "#    N_j = np.sum(Included, axis=1)\n",
    "#    mu_j = np.empty([M])\n",
    "#    std_j = np.empty([M])\n",
    "#    for j in range(M):\n",
    "#        mu_j[j] = np.mean(Residuals[j,:][Included[j,:]==1])\n",
    "#        std_j[j] = np.std(Residuals[j,:][Included[j,:]==1])\n",
    "#    Rescaled_Residuals = (Residuals - mu_j[:, np.newaxis])/std_j[:, np.newaxis]\n",
    "#    Z_train = 1/M_train*np.sum(Rescaled_Residuals*Included, axis=0)\n",
    "#    Z_infer = 1/(M-M_train)*np.sum(Rescaled_Residuals*NotIncluded, axis=0)\n",
    "#    S_USDR = Z_infer - Z_train\n",
    "#    S_USDR = minmax_scale(S_USDR, feature_range=(0,1))\n",
    "#\n",
    "#    return S_USDR, Z_infer, Z_train, Rescaled_Residuals, std_j, mu_j, Included, NotIncluded, Y_hat, Y\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "coreset_sampling_ratio = 0.01\n",
    "run = 1\n",
    "category = \"screw\"#\"wood\" #\"cable\" #\"metal_nut\" #  \n",
    "cont_ratio = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d392cd6531a4341821147c8648aaf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:129: `training_step` returned `None`. If this was on purpose, ignore this warning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e95857c9f3141d8ac174268e235fc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa1dd21a1c149c1af1a7c4bbc429ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "[rank: 0] Seed set to 1\n",
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038d5427b3b04ee59cd4085f4bdf6e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S_blind, Y_blind = train_and_predict_on_training_set_blind(coreset_sampling_ratio, run, category, cont_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932cb7c694cf45248850bf25f0ac95fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7faa3a200434dfcaf457948faab3e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defa8df98206497e91d2aa9800c9728e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "[rank: 0] Seed set to 1\n",
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a41678284be499b931555ec5b883824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S_clean, Y_clean = train_and_predict_on_training_set_clean(coreset_sampling_ratio, run, category, cont_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "[rank: 0] Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " ...\n",
      " [1 0 0 ... 1 1 1]\n",
      " [1 1 0 ... 0 1 1]\n",
      " [1 1 1 ... 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668754eb83be4c0288db4b4cbc066827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a57ae1483f043429487a3ebedee6ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cbc6de1d944458b7459750b89ed536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837bce1f7fbf4373a86df14fbb3f574f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5f0010cbe343978b7b36a61cb3b978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97d38ec95954f3e9c942ab3c01a470f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18a6d48b802469f83055785338ca19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8911f203747488e9a45bab97acc83c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29deae877d9949e59638aa2f23770d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6a2fa35dce42598a566a4c6db403e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be75dc844a4b4f0b98a0ff4633c4f40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2726b2203434cc88a1bfed0f37b23ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3175978903944fe784ece8ee4e51549c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5ed8551dfd490e91d86da1dfefdc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623dee8a199d4c0d8f5cf72a3e95177c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a63a1c015824f79875d785f0ff7e507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc75603092724cbe859ab4389225dc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc402115044f4464a30ee54c9c934439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818f2f474df342beb930e8acdfcb19ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df6dbe7b0bb411ebe5fdeb5ad5ce2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd98c3d156f4b22ac38bedf8a16ee1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5219952b81864dc5afb80a3a4a456c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c5e4ca869540ab8b9baf9d8eebd5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cecdc9cd45442bfa2a425b68545f50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S_USDR, S_USDR_minmax, Z_infer, Z_train, Rescaled_Residuals, std_j, mu_j, Included, NotIncluded, Y_hat, Y, S_ensemble, S_ensemble_max = train_and_predict_on_training_set_USDR(coreset_sampling_ratio, run, category, cont_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S_USDR = np.array([-1 , 1, 2, 3])\n",
    "# from sklearn.preprocessing import minmax_scale\n",
    "# \n",
    "# S_USDR_scaled = minmax_scale(S_USDR, feature_range=(0,1))\n",
    "# S_USDR_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(S_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_ensemble_old = S_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_ensemble = S_ensemble_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_ensemble = S_ensemble_max #S_ensemble = S_ensemble_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_ensemble ==S_ensemble_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_USDR = S_USDR_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_USDR_old = S_USDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "S_USDR = S_USDR_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S_ensemble = minmax_scale(S_ensemble, feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Average Precision \n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, PrecisionRecallDisplay\n",
    "\n",
    "# Blind\n",
    "precision_blind, recall_blind, _ = precision_recall_curve(Y_blind, S_blind)\n",
    "AP_blind = average_precision_score(Y_blind, S_blind)\n",
    "\n",
    "# Ensemble Blind\n",
    "precision_ensemble, recall_ensemble, _ = precision_recall_curve(Y, S_ensemble)\n",
    "AP_ensemble = average_precision_score(Y, S_ensemble)\n",
    "\n",
    "# USDR\n",
    "precision_USDR, recall_USDR, _ = precision_recall_curve(Y, S_USDR)\n",
    "AP_USDR = average_precision_score(Y, S_USDR)\n",
    "\n",
    "# Clean\n",
    "precision_clean, recall_clean, _ = precision_recall_curve(Y_clean, S_clean)\n",
    "AP_clean = average_precision_score(Y_clean, S_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu, threshold_triangle, threshold_minimum\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# https://bioimagebook.github.io/chapters/2-processing/3-thresholding/thresholding.html\n",
    "thresh_method = \"minimum\"\n",
    "\n",
    "if thresh_method==\"otsu\":\n",
    "    # Compute Otsu threshold\n",
    "    thresh_blind = threshold_otsu(S_blind)\n",
    "    thresh_ensemble = threshold_otsu(S_ensemble)\n",
    "    thresh_USDR = threshold_otsu(S_USDR)\n",
    "    thresh_clean = threshold_otsu(S_clean)\n",
    "elif thresh_method==\"triangle\":\n",
    "    # Compute triangle threshold\n",
    "    thresh_blind = threshold_triangle(S_blind)\n",
    "    thresh_ensemble = threshold_triangle(S_ensemble)\n",
    "    thresh_USDR = threshold_triangle(S_USDR)\n",
    "    thresh_clean = threshold_triangle(S_clean)\n",
    "elif thresh_method==\"minimum\":\n",
    "    # Compute minimum threshold\n",
    "    thresh_blind = threshold_minimum(S_blind)\n",
    "    thresh_ensemble = threshold_minimum(S_ensemble)\n",
    "    thresh_USDR = threshold_minimum(S_USDR)\n",
    "    thresh_clean = threshold_minimum(S_clean)\n",
    "else:\n",
    "    print(\"Method not implemented\")\n",
    "\n",
    "# Estimate anomaly ratio\n",
    "qratio_blind = np.sum(S_blind>thresh_blind) / len(S_blind)\n",
    "qratio_ensemble = np.sum(S_ensemble>thresh_ensemble) / len(S_ensemble)\n",
    "qratio_USDR = np.sum(S_USDR>thresh_USDR) / len(S_USDR)\n",
    "qratio_clean = np.sum(S_clean>thresh_clean) / len(S_clean)\n",
    "\n",
    "# # Classify as normal or abnormal based on threshold\n",
    "# Y_blind_thresh = np.zeros(len(S_blind))\n",
    "# Y_blind_thresh[S_blind>thresh_blind] = 1\n",
    "# Y_ensemble_thresh = np.zeros(len(S_ensemble))\n",
    "# Y_ensemble_thresh[S_ensemble>thresh_ensemble] = 1\n",
    "# Y_USDR_thresh = np.zeros(len(S_USDR))\n",
    "# Y_USDR_thresh[S_USDR>thresh_USDR] = 1\n",
    "# Y_clean_thresh = np.zeros(len(S_clean))\n",
    "# Y_clean_thresh[S_clean>thresh_clean] = 1\n",
    "\n",
    "# Classify as normal or abnormal based on threshold\n",
    "safety_factor = 1\n",
    "qthresh_blind = np.quantile(S_blind, 1-safety_factor*qratio_blind)\n",
    "qthresh_ensemble = np.quantile(S_ensemble, 1-safety_factor*qratio_ensemble)\n",
    "qthresh_USDR = np.quantile(S_USDR, 1-safety_factor*qratio_USDR)\n",
    "qthresh_clean = np.quantile(S_clean, 1-safety_factor*qratio_clean)\n",
    "Y_blind_thresh = np.where(S_blind < qthresh_blind, 0, 1)\n",
    "Y_ensemble_thresh = np.where(S_ensemble < qthresh_ensemble, 0, 1)\n",
    "Y_USDR_thresh = np.where(S_USDR < qthresh_USDR, 0, 1)\n",
    "Y_clean_thresh = np.where(S_clean < qthresh_clean, 0, 1)\n",
    "\n",
    "# Compute precision and recall\n",
    "precision_blind_thresh = precision_score(Y_blind, Y_blind_thresh)\n",
    "recall_blind_thresh = recall_score(Y_blind, Y_blind_thresh)\n",
    "precision_ensemble_thresh = precision_score(Y, Y_ensemble_thresh)\n",
    "recall_ensemble_thresh = recall_score(Y, Y_ensemble_thresh)\n",
    "precision_USDR_thresh = precision_score(Y, Y_USDR_thresh)\n",
    "recall_USDR_thresh = recall_score(Y, Y_USDR_thresh)\n",
    "precision_clean_thresh = precision_score(Y_clean, Y_clean_thresh)\n",
    "recall_clean_thresh = recall_score(Y_clean, Y_clean_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Plot setttings\n",
    "fontsize = 8\n",
    "markersize = 2\n",
    "\n",
    "fig, axs = plt.subplots(1, 7, figsize=(1.5*14,1.5*2), constrained_layout=True)\n",
    "\n",
    "# Blind\n",
    "axs[0].grid()\n",
    "axs[0].hist(S_blind[Y_blind==0], bins=100, range=(0,1), label=\"True normal\", color='C8')\n",
    "axs[0].hist(S_blind[Y_blind==1], bins=100, range=(0,1), label=\"True abnormal\", color='C5')\n",
    "axs[0].axvline(thresh_blind, label=\"Threshold\", color='C7')\n",
    "axs[0].axvline(qthresh_blind, label=\"Applied\", color='C7')\n",
    "axs[0].legend(fontsize=fontsize)\n",
    "axs[0].set_title(\"Blind\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[0].set_ylim([0, 50])\n",
    "axs[0].set_xlabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[0].set_ylabel(\"Count [-]\", fontsize=fontsize)\n",
    "axs[0].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[0].yaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "# Blind Ensemble \n",
    "axs[1].grid()\n",
    "axs[1].hist(S_ensemble[Y==0], bins=100, range=(0,1), label=\"True normal\", color='C8')\n",
    "axs[1].hist(S_ensemble[Y==1], bins=100, range=(0,1), label=\"True abnormal\", color='C5')\n",
    "axs[1].axvline(thresh_ensemble, label=\"Threshold\", color='C7', linestyle='--')\n",
    "axs[1].axvline(qthresh_ensemble, label=\"Applied\", color='C7')\n",
    "axs[1].legend(fontsize=fontsize)\n",
    "axs[1].set_title(\"Ensemble\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[1].set_ylim([0, 50])\n",
    "axs[1].set_xlabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[1].set_ylabel(\"Count [-]\", fontsize=fontsize)\n",
    "axs[1].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[1].yaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "# USDR\n",
    "axs[2].grid()\n",
    "axs[2].hist(S_USDR[Y==0], bins=100, range=(0,1), label=\"True normal\", color='C8')\n",
    "axs[2].hist(S_USDR[Y==1], bins=100, range=(0,1), label=\"True abnormal\", color='C5')\n",
    "axs[2].axvline(thresh_USDR, label=\"Threshold\", color='C7', linestyle='--')\n",
    "axs[2].axvline(qthresh_USDR, label=\"Applied\", color='C7')\n",
    "axs[2].legend(fontsize=fontsize)\n",
    "axs[2].set_title(\"USDR\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[2].set_ylim([0, 50])\n",
    "axs[2].set_xlabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[2].set_ylabel(\"Count [-]\", fontsize=fontsize)\n",
    "axs[2].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[2].yaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "# Clean\n",
    "axs[3].grid()\n",
    "axs[3].hist(S_clean[Y_clean==0], bins=100, range=(0,1), label=\"True normal\", color='C8')\n",
    "axs[3].hist(S_clean[Y_clean==1], bins=100, range=(0,1), label=\"True abnormal\", color='C5')\n",
    "axs[3].axvline(thresh_clean, label=\"Threshold\", color='C7', linestyle='--')\n",
    "axs[3].axvline(qthresh_clean, label=\"Applied\", color='C7')\n",
    "axs[3].legend(fontsize=fontsize)\n",
    "axs[3].set_title(\"Clean\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[3].set_ylim([0, 50])\n",
    "axs[3].set_xlabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[3].set_ylabel(\"Count [-]\", fontsize=fontsize)\n",
    "axs[3].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[3].yaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "# Estimated anomaly ratio\n",
    "axs[4].bar([\"Blind\", \"Ensemble\", \"USDR\", \"Clean\"], [qratio_blind, qratio_ensemble, qratio_USDR, qratio_clean], color=['C0', 'C2', 'C3', 'C1'])#, color=['C7', 'C7', 'C7', 'C7'])\n",
    "axs[4].axhline(cont_ratio, label=\"True anomaly ratio\", linestyle='--', color='black')\n",
    "axs[4].legend(fontsize=fontsize)\n",
    "axs[4].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[4].yaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[4].set_ylabel(\"Estimated anomaly ratio [-]\", fontsize=fontsize)\n",
    "axs[4].set_ylim([-0.05, 1.05])\n",
    "#axs[4].set_ylim([-0.025, 0.50])\n",
    "\n",
    "# Precision Recall Curves\n",
    "#disp = PrecisionRecallDisplay(precision=precision_blind, recall=recall_blind)\n",
    "axs[5].plot(recall_blind, precision_blind, label=\"$S^{blind}$\", color='C0')\n",
    "axs[5].plot(recall_blind_thresh, precision_blind_thresh, color='C0', marker='D', markersize=3*markersize)\n",
    "axs[5].plot(recall_ensemble, precision_ensemble, label=\"$S^{ensemble}$\", color='C2')\n",
    "axs[5].plot(recall_ensemble_thresh, precision_ensemble_thresh, color='C2', marker='D', markersize=3*markersize)\n",
    "axs[5].plot(recall_USDR, precision_USDR, label=\"$S^{USDR}$\", color='C3')\n",
    "axs[5].plot(recall_USDR_thresh, precision_USDR_thresh, color='C3', marker='D', markersize=3*markersize)\n",
    "axs[5].plot(recall_clean, precision_clean, label=\"$S^{clean}$\", color='C1')\n",
    "axs[5].plot(recall_clean_thresh, precision_clean_thresh, color='C1', marker='D', markersize=3*markersize)\n",
    "axs[5].legend(fontsize=fontsize)\n",
    "axs[5].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[5].yaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[5].set_xlabel(\"Recall [-]\", fontsize=fontsize)\n",
    "axs[5].set_ylabel(\"Precision [-]\", fontsize=fontsize)\n",
    "axs[5].set_xlim([-0.05, 1.05])\n",
    "axs[5].set_ylim([-0.05, 1.05])\n",
    "\n",
    "\n",
    "# Average Precision Scores Barplot\n",
    "x = [\"Blind\", \"Ensemble\", \"USDR\", \"Clean\"]\n",
    "y = [AP_blind, AP_ensemble, AP_USDR, AP_clean]\n",
    "axs[6].bar(x, y, color=['C0', 'C2', 'C3', 'C1'])\n",
    "for i, v in enumerate(y):\n",
    "    axs[6].text(i, v + 0.01, str(round(v,3)), ha='center', fontsize=fontsize)\n",
    "\n",
    "axs[6].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[6].yaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[6].set_ylabel(\"Avg. Precision [-]\", fontsize=fontsize)\n",
    "axs[6].set_ylim([-0.05, 1.05])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_USDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_ensemble\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Blind\n",
    "#axs[0].plot(S_blind, label=\"$S^{blind}$\", color='grey', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[0].plot(np.where(Y_blind==0)[0], Y_blind[Y_blind==0], label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[0].plot(np.where(Y_blind==1)[0], Y_blind[Y_blind==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[0].legend(fontsize=fontsize)\n",
    "#axs[0].grid()\n",
    "#axs[0].xaxis.set_tick_params(labelsize=fontsize)\n",
    "#axs[0].yaxis.set_tick_params(labelsize=fontsize)\n",
    "#axs[0].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "#axs[0].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "#\n",
    "#\n",
    "## Blind Ensemble \n",
    "#axs[1].plot(S_ensemble, label=\"$S^{ensemble}$\", color='grey', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[1].plot(np.where(Y==0)[0], Y[Y==0], label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[1].plot(np.where(Y==1)[0], Y[Y==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[1].legend(fontsize=fontsize)\n",
    "#axs[1].grid()\n",
    "#axs[1].xaxis.set_tick_params(labelsize=fontsize)\n",
    "#axs[1].yaxis.set_tick_params(labelsize=fontsize)\n",
    "#axs[1].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "#axs[1].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "#\n",
    "## USDR\n",
    "#axs[2].plot(S_USDR, label=\"$S^{USDR}$\", color='grey', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[2].plot(np.where(Y==0)[0], Y[Y==0], label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[2].plot(np.where(Y==1)[0], Y[Y==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[2].legend(fontsize=fontsize)\n",
    "#axs[2].grid()\n",
    "#axs[2].xaxis.set_tick_params(labelsize=fontsize)\n",
    "#axs[2].yaxis.set_tick_params(labelsize=fontsize)\n",
    "#axs[2].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "#axs[2].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "#\n",
    "#\n",
    "## Clean\n",
    "#axs[3].plot(S_clean, label=\"$S^{clean}$\", color='grey', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[3].plot(np.where(Y_clean==0)[0], Y_clean[Y_clean==0], label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[3].plot(np.where(Y_clean==1)[0], Y_clean[Y_clean==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "#axs[3].legend(fontsize=fontsize)\n",
    "#axs[3].grid()\n",
    "#axs[3].xaxis.set_tick_params(labelsize=fontsize)\n",
    "#axs[3].yaxis.set_tick_params(labelsize=fontsize)\n",
    "#axs[3].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "#axs[3].set_ylabel(\"Score [-]\", fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(10, 2.5), constrained_layout=True)\n",
    "\n",
    "# Blind\n",
    "axs[0].grid()\n",
    "axs[0].hist(S_blind[Y_blind==0], bins=100, range=(0,1), label=\"True normal\", color='C8')\n",
    "axs[0].hist(S_blind[Y_blind==1], bins=100, range=(0,1), label=\"True abnormal\", color='C5')\n",
    "axs[0].axvline(thresh_blind, label=\"Otsu\", color='C7')\n",
    "axs[0].legend(fontsize=fontsize)\n",
    "axs[0].set_ylim([0, 50])\n",
    "axs[0].set_xlabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[0].set_ylabel(\"Count [-]\", fontsize=fontsize)\n",
    "axs[0].set_title(\"Blind (\" + str(round(qratio_blind,3)) + \")\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[0].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[0].yaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "# Blind Ensemble \n",
    "axs[1].grid()\n",
    "axs[1].hist(S_ensemble[Y==0], bins=100, range=(0,1), label=\"True normal\", color='C8')\n",
    "axs[1].hist(S_ensemble[Y==1], bins=100, range=(0,1), label=\"True abnormal\", color='C5')\n",
    "axs[1].axvline(threshold_otsu(S_ensemble), label=\"Otsu\", color='C7')\n",
    "axs[1].legend(fontsize=fontsize)\n",
    "axs[1].set_ylim([0, 50])\n",
    "axs[1].set_xlabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[1].set_ylabel(\"Count [-]\", fontsize=fontsize)\n",
    "axs[1].set_title(\"Ensemble (\" + str(round(qratio_ensemble,3)) + \")\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[1].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[1].yaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "# USDR\n",
    "axs[2].grid()\n",
    "axs[2].hist(S_USDR[Y==0], bins=100, range=(0,1), label=\"True normal\", color='C8')\n",
    "axs[2].hist(S_USDR[Y==1], bins=100, range=(0,1), label=\"True abnormal\", color='C5')\n",
    "axs[2].axvline(thresh_USDR, label=\"Otsu\", color='C7')\n",
    "axs[2].legend(fontsize=fontsize)\n",
    "axs[2].set_ylim([0, 50])\n",
    "axs[2].set_xlabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[2].set_ylabel(\"Count [-]\", fontsize=fontsize)\n",
    "axs[2].set_title(\"USDR (\" + str(round(qratio_USDR,3)) + \")\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[2].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[2].yaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "# Clean\n",
    "axs[3].grid()\n",
    "axs[3].hist(S_clean[Y_clean==0], bins=100, range=(0,1), label=\"True normal\", color='C8')\n",
    "axs[3].hist(S_clean[Y_clean==1], bins=100, range=(0,1), label=\"True abnormal\", color='C5')\n",
    "axs[3].axvline(thresh_clean, label=\"Otsu\", color='C7')\n",
    "axs[3].legend(fontsize=fontsize)\n",
    "axs[3].set_ylim([0, 50])\n",
    "axs[3].set_xlabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[3].set_ylabel(\"Count [-]\", fontsize=fontsize)\n",
    "axs[3].set_title(\"Clean (\" + str(round(qratio_clean,3)) + \")\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[3].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[3].yaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute USDR scores\n",
    "#\n",
    "#Y = label_arr \n",
    "#Y_hat = pred_scores_arr\n",
    "#Residuals = abs(Y_hat) # abs(Y - Y_hat)\n",
    "#Included = subsets_bool_sorted_arr\n",
    "#NotIncluded = 1-subsets_bool_sorted_arr\n",
    "#N_j = np.sum(Included, axis=1)\n",
    "#\n",
    "#mu_j = np.empty([M])\n",
    "#std_j = np.empty([M])\n",
    "#for j in range(M):\n",
    "#    mu_j[j] = np.mean(Residuals[j,:][Included[j,:]==1])\n",
    "#    std_j[j] = np.std(Residuals[j,:][Included[j,:]==1])\n",
    "#\n",
    "#Rescaled_Residuals = (Residuals - mu_j[:, np.newaxis])/std_j[:, np.newaxis]\n",
    "#Z_train = 1/M_train*np.sum(Rescaled_Residuals*Included, axis=0)\n",
    "#Z_infer = 1/(M-M_train)*np.sum(Rescaled_Residuals*NotIncluded, axis=0)\n",
    "#S_USDR = Z_infer - Z_train\n",
    "#\n",
    "#print(Y.shape)\n",
    "#print(Y_hat.shape)\n",
    "#print(Residuals.shape)\n",
    "#print(Included.shape)\n",
    "#print(N_j.shape)\n",
    "#print(mu_j.shape)\n",
    "#print(std_j.shape)\n",
    "#print(Rescaled_Residuals.shape)\n",
    "#print(Z_train.shape)\n",
    "#print(Z_infer.shape)\n",
    "#print(S_USDR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Plot setttings\n",
    "fontsize = 8\n",
    "markersize = 2\n",
    "\n",
    "M, N = Y_hat.shape\n",
    "\n",
    "fig, axs = plt.subplots(M, 4, figsize=(12,2*M), constrained_layout=True)\n",
    "for m_iter in range(M):\n",
    "\n",
    "\n",
    "    # Anomaly scores\n",
    "    axs[m_iter,0].plot(np.where(NotIncluded[m_iter,:]==1)[0], Y_hat[m_iter,:][NotIncluded[m_iter,:]==1], label=\"Not in subset\", color=\"grey\", linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m_iter,0].plot(np.where(Included[m_iter,:]==1)[0], Y_hat[m_iter,:][Included[m_iter,:]==1], label=\"In subset\", color=\"C9\", linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m_iter,0].plot(np.where(Y==0)[0], Y[Y==0], label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m_iter,0].plot(np.where(Y==1)[0], Y[Y==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m_iter,0].hlines(mu_j[m_iter], 0, N, color=\"C9\", linestyle='-')\n",
    "    axs[m_iter,0].fill_between([0, N], mu_j[m_iter] - 2*std_j[m_iter], mu_j[m_iter] + 2*std_j[m_iter], color=\"C9\", alpha=0.3, linewidth=0.0)\n",
    "    axs[m_iter,0].set_ylim([-0.1, 1.1])\n",
    "    axs[m_iter,0].grid()\n",
    "    axs[m_iter,0].xaxis.set_tick_params(labelsize=fontsize)\n",
    "    axs[m_iter,0].yaxis.set_tick_params(labelsize=fontsize)\n",
    "    axs[m_iter,0].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "    axs[m_iter,0].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "    # Rescaled residuals\n",
    "    axs[m_iter,1].plot(np.where(NotIncluded[m_iter,:]==1)[0], Rescaled_Residuals[m_iter,:][NotIncluded[m_iter,:]==1], label=\"Not in subset\", color='grey', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m_iter,1].plot(np.where(Included[m_iter,:]==1)[0], Rescaled_Residuals[m_iter,:][Included[m_iter,:]==1], label=\"In subset\", color='C9', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m_iter,1].plot(np.where(Y==0)[0], Y[Y==0]-5, label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m_iter,1].plot(np.where(Y==1)[0], 35*Y[Y==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m_iter,1].set_ylim([-10, 40])\n",
    "    axs[m_iter,1].grid()\n",
    "    axs[m_iter,1].xaxis.set_tick_params(labelsize=fontsize)\n",
    "    axs[m_iter,1].yaxis.set_tick_params(labelsize=fontsize)\n",
    "    axs[m_iter,1].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "    axs[m_iter,1].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "\n",
    "    if m_iter==0:\n",
    "        axs[m_iter,0].legend(fontsize=fontsize)\n",
    "\n",
    "        axs[m_iter,1].legend(fontsize=fontsize)\n",
    "\n",
    "        axs[m_iter,2].plot(Z_infer, label=\"$z^{infer}$\", color='grey', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,2].plot(Z_train, label=\"$z^{train}$\", color='C9', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,2].plot(np.where(Y==0)[0], Y[Y==0]-5, label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,2].plot(np.where(Y==1)[0], 35*Y[Y==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,2].set_ylim([-10, 40])\n",
    "        axs[m_iter,2].legend(fontsize=fontsize)\n",
    "        axs[m_iter,2].grid()\n",
    "        axs[m_iter,2].xaxis.set_tick_params(labelsize=fontsize)\n",
    "        axs[m_iter,2].yaxis.set_tick_params(labelsize=fontsize)\n",
    "        axs[m_iter,2].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "        axs[m_iter,2].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "        # USDR scores\n",
    "        axs[m_iter,3].plot(S_USDR, label=\"$S^{USDR}$\", color='grey', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,3].plot(np.where(Y==0)[0], Y[Y==0], label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,3].plot(np.where(Y==1)[0], Y[Y==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,3].set_ylim([-0.05, 1.05])\n",
    "        axs[m_iter,3].legend(fontsize=fontsize)\n",
    "        axs[m_iter,3].grid()\n",
    "        axs[m_iter,3].xaxis.set_tick_params(labelsize=fontsize)\n",
    "        axs[m_iter,3].yaxis.set_tick_params(labelsize=fontsize)\n",
    "        axs[m_iter,3].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "        axs[m_iter,3].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "\n",
    "        # Set titles\n",
    "        axs[m_iter,0].set_title(\"Anomaly scores\", fontsize=fontsize, fontweight=\"bold\")\n",
    "        axs[m_iter,1].set_title(\"Rescaled anomaly scores\", fontsize=fontsize, fontweight=\"bold\")\n",
    "        axs[m_iter,2].set_title(\"Mean scaled anomaly scores\", fontsize=fontsize, fontweight=\"bold\")\n",
    "        axs[m_iter,3].set_title(\"USDR scores\", fontsize=fontsize, fontweight=\"bold\")\n",
    "        \n",
    "    elif m_iter==1:\n",
    "        axs[m_iter,3].plot(S_ensemble, label=\"$S^{ensemble}$\", color='grey', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,3].plot(np.where(Y==0)[0], Y[Y==0], label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,3].plot(np.where(Y==1)[0], Y[Y==1], label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "        axs[m_iter,3].set_ylim([-0.05, 1.05])\n",
    "        axs[m_iter,3].legend(fontsize=fontsize)\n",
    "        axs[m_iter,3].grid()\n",
    "        axs[m_iter,3].xaxis.set_tick_params(labelsize=fontsize)\n",
    "        axs[m_iter,3].yaxis.set_tick_params(labelsize=fontsize)\n",
    "        axs[m_iter,3].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "        axs[m_iter,3].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "        axs[m_iter,3].set_title(\"Ensemble scores\", fontsize=fontsize, fontweight=\"bold\")\n",
    "        axs[1,2].axis('off')\n",
    "\n",
    "\n",
    "    elif m_iter>0:\n",
    "        # Mean residuals\n",
    "        axs[m_iter,2].axis('off')\n",
    "        axs[m_iter,3].axis('off')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_USDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\n",
    "\n",
    "y_true = Y\n",
    "y_scores = S_USDR\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_true, y_scores)\n",
    "\n",
    "m_iter = 0\n",
    "y_true = Y\n",
    "y_scores = S_USDR\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_true, y_scores)\n",
    "average_precision = average_precision_score(y_true, y_scores)\n",
    "disp = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=average_precision)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, average_precision_score\n",
    "### Precision Recall Curve\n",
    "\n",
    "m_iter = 1\n",
    "y_true = Y[NotIncluded[m_iter,:]==1]\n",
    "y_scores = Y_hat[m_iter,:][NotIncluded[m_iter,:]==1]\n",
    "#y_scores[y_scores > 0.49] = 1\n",
    "#y_scores[y_scores <= 0.49] = 0\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_true, y_scores)\n",
    "average_precision = average_precision_score(y_true, y_scores)\n",
    "disp = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=average_precision)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot setttings\n",
    "fontsize = 8\n",
    "markersize = 2\n",
    "\n",
    "# Prepare plot\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8,6), constrained_layout=True)\n",
    "\n",
    "for m_iter in  range(M):\n",
    "\n",
    "    # Prepare data\n",
    "    labels = np.array([item['label'].item() for item in predictions_subsets_arr[m_iter]])\n",
    "    labels0x = np.where(labels==0)[0]\n",
    "    labels0y = labels[labels==0]\n",
    "    labels1x = np.where(labels==1)[0]\n",
    "    labels1y = labels[labels==1]\n",
    "    \n",
    "    subsets_bool_sorted = subsets_bool_sorted_arr[m_iter]\n",
    "    pred_scores = np.array([item['pred_scores'][0] for item in predictions_subsets_arr[m_iter]])\n",
    "    \n",
    "    pred_scores_in_x = np.where(subsets_bool_sorted.astype(bool))[0]\n",
    "    pred_scores_in_y = pred_scores[subsets_bool_sorted.astype(bool)]\n",
    "    pred_scores_out_x = np.where(~subsets_bool_sorted.astype(bool))[0]\n",
    "    pred_scores_out_y = pred_scores[~subsets_bool_sorted.astype(bool)]\n",
    "\n",
    "    # Plot\n",
    "    m = 0#m_iter\n",
    "    axs[m].plot(labels0x, labels0y, label=\"True normal\", color='C2', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m].plot(labels1x, labels1y, label=\"True abnormal\", color='C3', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m].plot(pred_scores_in_x, pred_scores_in_y, label=\"Prediction Scores (in)\", color='C0', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    axs[m].plot(pred_scores_out_x, pred_scores_out_y, label=\"Prediction Scores (out)\", color='C1', linestyle='', marker=\"o\", markersize=markersize)\n",
    "\n",
    "    #axs[m_iter].plot(pred_scores, label=\"Prediction Scores\", color='C0', linestyle='', marker=\"o\", markersize=markersize)\n",
    "    #axs[m].legend(fontsize=fontsize)\n",
    "axs[m].grid()\n",
    "axs[m].set_title(\"Anomaly scores\", fontsize=fontsize, fontweight=\"bold\")\n",
    "axs[m].set_xlim([0, N])\n",
    "axs[m].set_xlabel(\"Index [-]\", fontsize=fontsize)\n",
    "axs[m].set_ylabel(\"Score [-]\", fontsize=fontsize)\n",
    "axs[m].xaxis.set_tick_params(labelsize=fontsize)\n",
    "axs[m].yaxis.set_tick_params(labelsize=fontsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
