{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/wueesmat/MT/anomalib')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_directory = Path.cwd()\n",
    "if current_directory.name == \"MT\":\n",
    "    # This means that the notebook is run from the main anomalib directory.\n",
    "    root_directory = current_directory.parent\n",
    "elif current_directory.name == \"anomalib\":\n",
    "    # This means that the notebook is run from the main anomalib directory.\n",
    "    root_directory = current_directory\n",
    "\n",
    "os.chdir(root_directory)\n",
    "root_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find wandb. To use this feature, ensure that you have wandb installed.\n",
      "Could not find openvino. To use this feature, ensure that you have openvino installed.\n",
      "OpenVINO is not installed. Please install OpenVINO to use OpenVINOInferencer.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Import the required modules\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from anomalib.data import MVTec\n",
    "from anomalib.data.image.mvtec import MVTec_contaminated, make_mvtec_dataset_contaminated\n",
    "from anomalib.models import Padim, Patchcore, Stfpm, Draem, EfficientAd\n",
    "from anomalib.engine import Engine\n",
    "from anomalib import TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  path  split    label  \\\n",
      "0    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train     good   \n",
      "1    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train     good   \n",
      "2    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train     good   \n",
      "3    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train     good   \n",
      "4    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train     good   \n",
      "..                                                 ...    ...      ...   \n",
      "253  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...   test     bent   \n",
      "254  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...   test    color   \n",
      "255  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...   test  scratch   \n",
      "256  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...   test    color   \n",
      "257  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...   test     flip   \n",
      "\n",
      "                                            image_path  label_index  \\\n",
      "0    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "1    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "2    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "3    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "4    /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "..                                                 ...          ...   \n",
      "253  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            1   \n",
      "254  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            1   \n",
      "255  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            1   \n",
      "256  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            1   \n",
      "257  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            1   \n",
      "\n",
      "                                             mask_path  \n",
      "0                                                       \n",
      "1                                                       \n",
      "2                                                       \n",
      "3                                                       \n",
      "4                                                       \n",
      "..                                                 ...  \n",
      "253  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  \n",
      "254  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  \n",
      "255  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  \n",
      "256  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  \n",
      "257  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  \n",
      "\n",
      "[258 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca0eb775dae482cbe86e172ab632de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:129: `training_step` returned `None`. If this was on purpose, ignore this warning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3d3e8960014aba8975668b8d0912c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "Restoring states from the checkpoint path at /home/wueesmat/MT/anomalib/results/Patchcore/MVTec_contaminated/metal_nut/v4/weights/lightning/model.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/wueesmat/MT/anomalib/results/Patchcore/MVTec_contaminated/metal_nut/v4/weights/lightning/model.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c49ec5356d14d8a8e9244b425b4cb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpt_path is not provided. Model weights will not be loaded.\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/wueesmat/anaconda3/envs/anomalib_env/lib/pytho ...\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df50ea881494948b29725f831af74eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 1\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/wueesmat/anaconda3/envs/anomalib_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:180: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model                 | PatchcoreModel           | 24.9 M\n",
      "1 | _transform            | Compose                  | 0     \n",
      "2 | normalization_metrics | MinMax                   | 0     \n",
      "3 | image_threshold       | F1AdaptiveThreshold      | 0     \n",
      "4 | pixel_threshold       | F1AdaptiveThreshold      | 0     \n",
      "5 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "6 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  path  split  label  \\\n",
      "179  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "54   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "204  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "112  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "222  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...   test  color   \n",
      "..                                                 ...    ...    ...   \n",
      "25   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "206  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "78   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "85   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "20   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  train   good   \n",
      "\n",
      "                                            image_path  label_index  \\\n",
      "179  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "54   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "204  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "112  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "222  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            1   \n",
      "..                                                 ...          ...   \n",
      "25   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "206  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "78   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "85   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "20   /home/wueesmat/MT/anomalib/datasets/MVTec/meta...            0   \n",
      "\n",
      "                                             mask_path  \n",
      "179                                                     \n",
      "54                                                      \n",
      "204                                                     \n",
      "112                                                     \n",
      "222  /home/wueesmat/MT/anomalib/datasets/MVTec/meta...  \n",
      "..                                                 ...  \n",
      "25                                                      \n",
      "206                                                     \n",
      "78                                                      \n",
      "85                                                      \n",
      "20                                                      \n",
      "\n",
      "[219 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcd39344ba042f6a37d497b89767ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb39ead70ca24e6d92196f4278eb916c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead\n",
      "Restoring states from the checkpoint path at /home/wueesmat/MT/anomalib/results/Patchcore/MVTec_contaminated/metal_nut/v5/weights/lightning/model.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/wueesmat/MT/anomalib/results/Patchcore/MVTec_contaminated/metal_nut/v5/weights/lightning/model.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27aedda6d38b4cde8d1c67f66ec96c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.15\n",
      "metal_nut\n"
     ]
    }
   ],
   "source": [
    "save_folder = \"./results/Patchcore/\"\n",
    "name = \"Patchcore_20240402_no_refinement.npy\"\n",
    "name_refined = \"Patchcore_20240402_simple_refinement.npy\"\n",
    "file_path = os.path.join(save_folder, name)\n",
    "file_path_refined = os.path.join(save_folder, name_refined)\n",
    "\n",
    "run_arr = np.array([1]) #np.array([1, 2, 3]) #np.array([42])#np.arange(3)+1\n",
    "cont_ratio_arr = np.array([0.15])#, 0.1]) #np.array([0.0, 0.05, 0.1]) #, 0.15\n",
    "category_arr = np.array([\"metal_nut\"])#, \"grid\"])#np.array([\"carpet\", \"grid\", \"leather\", \"tile\", \"wood\", \"bottle\", \"cable\", \"capsule\", \"hazelnut\", \"metal_nut\", \"pill\", \"screw\", \"toothbrush\", \"transistor\", \"zipper\"])#np.array([\"carpet\", \"grid\"]) #\n",
    "results_arr = np.empty([run_arr.shape[0], cont_ratio_arr.shape[0], category_arr.shape[0]])\n",
    "results_arr_refined = np.empty([run_arr.shape[0], cont_ratio_arr.shape[0], category_arr.shape[0]])\n",
    "\n",
    "for idx_run, run in enumerate(run_arr):\n",
    "    for idx_cont_ratio, cont_ratio in enumerate(cont_ratio_arr):\n",
    "        for idx_category, category in enumerate(category_arr):\n",
    "\n",
    "            # Define seed, datamodule, model and engine\n",
    "            seed_everything(run, workers=True)\n",
    "            datamodule = MVTec_contaminated(category=category, cont_ratio=cont_ratio, run=run, idx=[])\n",
    "            model = Patchcore() #Padim(backbone=\"resnet18\", n_features=100, layers=[\"layer1\", \"layer2\", \"layer3\"])\n",
    "            engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "\n",
    "            # Train the model\n",
    "            engine.fit(datamodule=datamodule, model=model)\n",
    "\n",
    "            # load best model from checkpoint before evaluating\n",
    "            test_results = engine.test(\n",
    "                model=model,\n",
    "                datamodule=datamodule,\n",
    "                ckpt_path=engine.trainer.checkpoint_callback.best_model_path,\n",
    "                verbose=False\n",
    "            )\n",
    "            results_arr[idx_run, idx_cont_ratio, idx_category] = test_results[0][\"image_AUROC\"]\n",
    "\n",
    "            \n",
    "            # Refine training set \n",
    "            prediction_dataset = datamodule.get_train_dataset()\n",
    "            predictions = engine.predict(model=model, dataset=prediction_dataset) # Make predictions on training set\n",
    "            prediction_scores = np.array([d[\"pred_scores\"][0] for d in predictions]).tolist() # Get list of prediction scores\n",
    "            sorted_indices = sorted(range(len(prediction_scores)), key=lambda i: prediction_scores[i]) # Sort the training samples based on prediction scores\n",
    "            selected_indices = sorted_indices[:int((1-cont_ratio) * len(sorted_indices))] # Select the 90% lowest prediction score samples\n",
    "\n",
    "            \n",
    "            # Define seed, datamodule, model and engine\n",
    "            seed_everything(run, workers=True)\n",
    "            datamodule_refined = MVTec_contaminated(category=category, cont_ratio=cont_ratio, run=run, idx=selected_indices)\n",
    "            model_refined = Patchcore()\n",
    "            engine_refined = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "\n",
    "            # Train the model\n",
    "            engine_refined.fit(datamodule=datamodule_refined, model=model_refined)\n",
    "\n",
    "            # load best model from checkpoint before evaluating\n",
    "            test_results_refined = engine_refined.test(\n",
    "                model=model_refined,\n",
    "                datamodule=datamodule_refined,\n",
    "                ckpt_path=engine_refined.trainer.checkpoint_callback.best_model_path,\n",
    "                verbose=False\n",
    "            )\n",
    "            results_arr_refined[idx_run, idx_cont_ratio, idx_category] = test_results_refined[0][\"image_AUROC\"]\n",
    "            \n",
    "            print(run)\n",
    "            print(cont_ratio)\n",
    "            print(category)\n",
    "\n",
    "# Save results_arr to the specified folder\n",
    "np.save(file_path, results_arr)\n",
    "np.save(file_path_refined, results_arr_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214285714285714"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "258/280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9611570835113525"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results[0][\"image_AUROC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9719008207321167"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_refined[0][\"image_AUROC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model from checkpoint before evaluating\n",
    "test_results2 = engine2.test(\n",
    "    model=model2,\n",
    "    datamodule=datamodule2,\n",
    "    ckpt_path=engine2.trainer.checkpoint_callback.best_model_path,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results2[0][\"image_AUROC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results4 = engine2.test(\n",
    "    model=model2,\n",
    "    datamodule=datamodule2,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results4[0][\"image_AUROC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores_array = np.array([d[\"pred_scores\"] for d in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.get_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_scores_array, bins=5)  # Adjust the number of bins as needed\n",
    "plt.title('Histogram of Prediction Scores')\n",
    "plt.xlabel('Prediction Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['pred_scores'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores_array = np.array([d[\"pred_scores\"][0] for d in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([d[\"pred_scores\"][0] for d in predictions]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of prediction scores\n",
    "prediction_scores = np.array([d[\"pred_scores\"][0] for d in predictions]).tolist()\n",
    "\n",
    "# Sort the training samples based on prediction scores\n",
    "sorted_indices = sorted(range(len(prediction_scores)), key=lambda i: prediction_scores[i])\n",
    "\n",
    "# Select the 90% lowest prediction score samples\n",
    "selected_indices = sorted_indices[:int(0.9 * len(sorted_indices))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset with the selected samples\n",
    "new_train_data = [datamodule.train_data[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule2 = datamodule\n",
    "datamodule2.set_train_data(new_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(run, workers=True)\n",
    "datamodule = MVTec_contaminated(category=category, cont_ratio=cont_ratio, run=run)\n",
    "\n",
    "datamodule2 = datamodule\n",
    "datamodule2.set_train_data(new_train_data)\n",
    "\n",
    "#model = Padim(backbone=\"resnet18\", n_features=100, layers=[\"layer1\", \"layer2\", \"layer3\"])\n",
    "model = Patchcore()\n",
    "\n",
    "engine = Engine(task=TaskType.CLASSIFICATION, image_metrics=[\"AUROC\", \"AUPR\", \"F1Score\"], max_epochs=10, devices=1)\n",
    "\n",
    "# Train the model\n",
    "engine.fit(datamodule=datamodule2, model=model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction_dataset = datamodule.get_train_dataset()\n",
    "predictions = engine.predict(model=model, dataset=new_prediction_dataset)\n",
    "\n",
    "pred_scores_array = np.array([d[\"pred_scores\"] for d in predictions])\n",
    "np.percentile(pred_scores_array, q=100*(1-cont_ratio))\n",
    "\n",
    "\n",
    "#######\n",
    "# load best model from checkpoint before evaluating\n",
    "test_results = engine.test(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=engine.trainer.checkpoint_callback.best_model_path,\n",
    "    verbose=False\n",
    ")\n",
    "results_arr[idx_run, idx_cont_ratio, idx_category] = test_results[0][\"image_AUROC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(pred_scores_array, q=100*(1-cont_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores_array = np.array([d[\"pred_scores\"] for d in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d[\"pred_scores\"] for d in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_predictions = sorted(predictions, key=lambda x: x[\"pred_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_predictions[0][\"pred_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_predictions[270][\"pred_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
